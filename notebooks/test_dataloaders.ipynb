{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dataloaders\n",
    "\n",
    "this notebook is to test the text-dataloaders for the translation task for the transformer model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pritishmishra/anaconda3/envs/transformer_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# adding project root directory to the sys path\n",
    "project_root = os.path.abspath(os.path.join(\n",
    "    os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.data.text_dataloader import TextDataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the downloaded datasets. \n",
    "## here, we will use the bert-base-multilingual-uncased tokenizer for the tokenization of the text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No, no, not so fast.\\n', ', eject!\\n', \"I'm Dr. Messa.\\n\", 'So we notify the cops about big ticket sales and we even keep half a dozen Ukrainian ex-naval commandos in a van outside, just in case it all kicks off.\\n', 'receiving what their Lord has given them, for they had been virtuous aforetime.\\n', \"Default folder to use for the '--add' and '--extract' commands\\n\", 'Hey, how are you? Beautiful day.\\n', 'Dengue is a tropical virus carried by the Aedes aegypti mosquito with no known cure. According to the World Health Organization, about 40 percent of the world’s population is at risk from dengue.\\n', 'Show right margin\\n', '%s: not enough free space\\n'] \n",
      "\n",
      "['तुम इतनी आसानी से छूट नहीं सकते.\\n', ', बेदखल!\\n', 'Messa हूँ.\\n', 'तोहमबड़ीटिकटोंकीबिक्रीकेबारे मेंपुलिस सूचित... / मैं ... और हम भी रखना आधा दर्जन यूक्रेनी पूर्व नौसेना कमांडो...\\n', 'जो कुछ उनके रब ने उन्हें दिया, वे उसे ले रहे होंगे। निस्संदेह वे इससे पहले उत्तमकारों में से थे\\n', '--add और --extract कमान्ड में उपयोग हेतु डिफ़ॉल्ट फ़ोल्डर\\n', 'सुहाना दिन है।\\n', '5 प्रतिशत कम हो गई, यानि डेंगू के कारण अस्पताल में भर्ती होने की आशंका में करीब 67 प्रतिशत कमी आई।\\n', 'दिखाएँ दायाँ\\n', '%s: पर्याप्त मुक्त स्थान नहींFree\\n'] \n",
      "\n",
      "Total number of samples in the source data: 2000 \n",
      "\n",
      "Total number of samples in the target data: 2000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_data = os.path.join(project_root, 'Datasets', 'raw', 'en-hi', 'opus.en-hi-dev.en')\n",
    "tgt_data = os.path.join(project_root, 'Datasets', 'raw', 'en-hi', 'opus.en-hi-dev.hi')\n",
    "\n",
    "# print the first 10 lines of the source and target data\n",
    "with open(src_data, 'r') as f:\n",
    "    print(f.readlines()[:10], \"\\n\")\n",
    "\n",
    "with open(tgt_data, 'r') as f:\n",
    "    print(f.readlines()[:10], \"\\n\")\n",
    "\n",
    "\n",
    "# print the total number of samples in the source and target data\n",
    "print(f\"Total number of samples in the source data: {len(open(src_data, 'r').readlines())}\", \"\\n\")\n",
    "print(f\"Total number of samples in the target data: {len(open(tgt_data, 'r').readlines())}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and test the test the text dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextDataset.__init__() missing 1 required positional argument: 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m text_dataloader \u001b[38;5;241m=\u001b[39m TextDataLoader(src_data, tgt_data)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# load the data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m src_data, tgt_data \u001b[38;5;241m=\u001b[39m \u001b[43mtext_dataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print the shapes of the source and target data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of the source data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/machine learning/transformer/src/data/text_dataloader.py:67\u001b[0m, in \u001b[0;36mTextDataLoader.load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03moverriding the load_data() method defined in the BaseDataLoader class\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Create TextDataset instances for source and target data\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m src_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTextDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_text_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m tgt_dataset \u001b[38;5;241m=\u001b[39m TextDataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_text_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_tokenizer, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Combine source and target datasets into a single Tensor Dataset\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: TextDataset.__init__() missing 1 required positional argument: 'tokenizer'"
     ]
    }
   ],
   "source": [
    "# initialize the text dataloader\n",
    "text_dataloader = TextDataLoader(src_data, tgt_data)\n",
    "# load the data\n",
    "src_data, tgt_data = text_dataloader.load_data()\n",
    "# print the shapes of the source and target data\n",
    "print(f\"Shape of the source data: {src_data.shape}\", \"\\n\")\n",
    "print(f\"Shape of the target data: {tgt_data.shape}\", \"\\n\")\n",
    "\n",
    "# print the first 10 samples of the source and target data\n",
    "print(f\"First 10 samples of the source data: {src_data[:10]}\", \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_transformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
